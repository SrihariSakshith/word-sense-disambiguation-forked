{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Lesk Algorithm (Method 2)\n",
    "\n",
    "__Anish Sachdeva (DTU/2K16/MC/013)__\n",
    "\n",
    "__Natural Language Processing (Dr. Seba Susan)__\n",
    "\n",
    "In teh simple LESK agorithm we are given the gloss, which is the surrounding words/tokens of a given word and we need to disambiguate the given word using the gloss. We will calculate the IDF (Inverse document Frequency) of all the words in the gloss and then assign weights to all possible senses that the word can have and then disambiguate the word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing the required packages\n",
    "import pprint\n",
    "\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "# nltk.download('wordnet')\n",
    "from nltk.corpus import stopwords\n",
    "# nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now define the `tokenize` method which takes in a string and returns the tokenized form with the word and stopwords removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords_en = set(stopwords.words('english'))\n",
    "\n",
    "\n",
    "def tokenize(document: str, word: str) -> set:\n",
    "    # obtaining tokens from the gloss\n",
    "    tokenizer = nltk.RegexpTokenizer(r'\\w+')\n",
    "    tokens = tokenizer.tokenize(document)\n",
    "\n",
    "    # removing stop words from tokens\n",
    "    tokens = [token for token in tokens if token not in stopwords_en and token.isalpha()]\n",
    "\n",
    "    # removing the word from the tokens\n",
    "    tokens = [token for token in tokens if token != word]\n",
    "    return set(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now define the simple LESK Algorithm which will take in the gloss and the word and will return the disambiguated sense from the wordnet corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_lesk(gloss: str, word: str):\n",
    "    \"\"\"\":returns the sense most suited to the given word as per the Simple LESK Algorithm\"\"\"\n",
    "\n",
    "    # converting everything to lowercase\n",
    "    gloss = gloss.lower()\n",
    "    word = word.lower()\n",
    "\n",
    "    # obtaining tokens from the gloss\n",
    "    gloss_tokens = tokenize(gloss, word)\n",
    "\n",
    "    # calculating the word sense disambiguation using simple LESK\n",
    "    synsets = wordnet.synsets(word)\n",
    "    weights = [0] * len(synsets)\n",
    "    N_t = len(synsets)\n",
    "    N_w = {}\n",
    "\n",
    "    # Creating the IDF Frequency column using Laplacian Scaling\n",
    "    for gloss_token in gloss_tokens:\n",
    "        N_w[gloss_token] = 1\n",
    "\n",
    "        for sense in synsets:\n",
    "            if gloss_token in sense.definition():\n",
    "                N_w[gloss_token] += N_t\n",
    "                continue\n",
    "\n",
    "            for example in sense.examples():\n",
    "                if gloss_token in example:\n",
    "                    N_w[gloss_token] += N_t\n",
    "                    break\n",
    "\n",
    "    for index, sense in enumerate(synsets):\n",
    "        # adding tokens from examples into the comparison set\n",
    "        comparison = set()\n",
    "        for example in sense.examples():\n",
    "            for token in tokenize(example, word):\n",
    "                comparison.add(token)\n",
    "\n",
    "        # adding tokens from definition into the comparison set\n",
    "        for token in tokenize(sense.definition(), word):\n",
    "            comparison.add(token)\n",
    "\n",
    "        # comparing the gloss tokens with comparison set\n",
    "        for token in gloss_tokens:\n",
    "            if token in comparison:\n",
    "                weights[index] += np.log(N_w[token] / N_t)\n",
    "\n",
    "    max_weight = max(weights)\n",
    "    index = weights.index(max_weight)\n",
    "    return synsets[index], weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The disambiguated meaning is: a beverage consisting of an infusion of ground coffee beans\n",
      "The weight vector is: [0, 0.28768207245178085, 0]\n"
     ]
    }
   ],
   "source": [
    "# We now test this code with our own gloss and words, (You can modify the code below with any word or gloss of your choice\n",
    "gloss = 'I love me a hot cup of java in the morning'\n",
    "word = 'java'\n",
    "sense, weights = simple_lesk(gloss, word)\n",
    "print('The disambiguated meaning is:', sense.definition())\n",
    "print('The weight vector is:', weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The disambiguated meaning is: a platform-independent object-oriented programming language\n",
      "The weight vector is: [0, 0, 0.5753641449035617]\n"
     ]
    }
   ],
   "source": [
    "# another test\n",
    "gloss = 'java is my favourite programming language'\n",
    "sense, weights = simple_lesk(gloss, word)\n",
    "print('The disambiguated meaning is:', sense.definition())\n",
    "print('The weight vector is:', weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, using the simple LESK akgorithm we can disambiguate the meaning of a word given its gloss."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
